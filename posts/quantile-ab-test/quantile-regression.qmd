---
title: "分位数回归的原理、代码实现和聚类标准误"
date: "2023-09-18"
categories: [A/B test, quantile regression, clustered standard errors]
bibliography: references.bib
image: spotify.png
---

分位数回归(quantile regression)可以回答实验操作/处理对指标分布的影响，一个非常经典的例子是，教育培训对收入的影响，在这里，我们不仅关心平均收入，还关心收入的平等，比如收入的中位数等。下面我们简要概述分位数回归的原理，以及相关代码实现。

分位数回归的核心问题是估计条件分位数$Q_{\tau}(Y_i|X_i) = F_y^-1(\tau|X_i)$：对连续变量Y，在给定一组协变量X下，预测其第$tau$分位数。它也是以下问题的解:

$$ Q_{\tau}(Y_i|X_i) = \mathop{\arg\min}\limits_{q(X)} E[\rho_\tau(Y - q(X_i))] $$

记$\rho_\tau(u) = (\tau - \mathbb{1}(u \leq 0)) * u, u = Y - q(X_i)$，这个公式也是分位数回归的损失函数，我们可以在xgboost、lightgbm、pytorch中自定义损失函数，实现非线性/深度分位数回归模型，但这里我们只考虑线性的情况，对线性分位数回归，\beta_{\tau}满足以下条件

$$ \beta_{\tau} = \mathop{\arg\min}\limits_{q(X)} E[\rho_\tau(Y - X\beta)] $$

线性规划、内点法()、迭代加权最小二乘法(IRLS: Iteratively reweighted least squares)都可以估计$\beta_{\tau}$, R的[quantreg](https://github.com/cran/quantreg/)和[pyqreg](https://github.com/mozjay0619/pyqreg)采用了内点法, statsmodels的quantile_regression采用IRLS。IRLS的实现比较简单，下面是示例的代码。

```{python}
import numpy as np
import numpy.typing as npt
import warnings
import scipy.stats as stats
from scipy.stats import norm

def quantreg_by_irls(
    X: npt.NDArray[np.float64], 
    y: npt.NDArray[np.float64],
    q: float = 0.5,
    max_iter: int = 1000, 
    p_tol: float = 1e-6,
    r_tol: float = 1e-6
):
    beta = np.ones(X.shape[1])
    # beta0 = np.zeros(X.shape[1])
    # diff = np.max(np.abs(beta - beta0))
    xstar = X
    n_iter = 0
    while n_iter < max_iter:
        n_iter += 1
        beta0 = beta
        xtx = np.dot(xstar.T, X)
        xty = np.dot(xstar.T, y)
        beta = np.dot(np.linalg.pinv(xtx), xty)
        if np.max(np.abs(beta - beta0)) <= p_tol:
            break

        resid = y - np.dot(X, beta)
        # avoid dividing by zero
        mask = np.abs(resid) < r_tol
        resid[mask] = ((resid[mask] >= 0) * 2 - 1) * r_tol

        # check function是np.where(resid < 0, -(1 - q) * resid, q * resid), 这里1 - q和q反过来了？
        resid = np.where(resid < 0, -q * resid, (1 - q) * resid)
        xstar = X / resid[:, np.newaxis]
        
    if n_iter == max_iter:
        warnings.warn(f"Maximum number of iterations ({max_iter}) reached.")

    return beta

(n0, mu0, sd0) = (30000, 0.0, 1.0)
(n1, mu1, sd1) = (31000, 0.2, 0.8)
tau = 0.7
tq = norm.pdf(tau, mu1, sd1) - norm.pdf(tau, mu0, sd0)  # 真实的qte
y0 = np.random.normal(mu0, sd0, n0)
y1 = np.random.normal(mu1, sd1, n1) 

X = np.array([
  np.ones(n0 + n1),
  np.repeat([0, 1], [n0, n1])
]).T
y = np.concatenate([y0, y1])
beta = quantreg_by_irls(X, y, tau)
(q0, q1) = np.quantile(y0, tau), np.quantile(y1, tau) 
print(f"beta:{beta}, y0的tau分位数: {q0}, y1的tau分位数 - y0的tau分位数: {q1 - q0}")

```

我们模拟了$y \sim 1 + D$，其中D是0，1分组变量，在这种情况下，beta实际就是$[Q_{y_0}(\tau), Q_{y_1}(\tau) - Q_{y_0}(\tau)]$.

## 协方差矩阵估计

在残差满足i.i.d假设下，$\beta$的协方差矩阵渐进于以下公式：

$$ \sqrt{n}(\hat{\beta}(\tau) - {\beta}(\tau)) \sim \frac{\tau * (1 - \tau)}{f_{\mu_{\tau}}(0|X_i)}(X^{'}X)^{-1} $$, 

如果有异方差的问题

$$ \sqrt{n}(\hat{\beta}(\tau) - {\beta}(\tau)) \sim \frac{\tau * (1 - \tau)}{f_{\mu_{\tau}}(0|X_i)}(X^{'}X)^{-1} $$


```{python}
from typing import List, Callable
import numpy as np
import scipy.stats as stats
from scipy.stats import norm

def hall_sheather(n, tau, alpha=.05):
    z = norm.ppf(tau)
    num = 1.5 * norm.pdf(z)**2.
    den = 2. * z**2. + 1.
    h = n**(-1. / 3) * norm.ppf(1. - alpha / 2.)**(2./3) * (num / den)**(1./3)
    return h

# stat.model中quantreg的默认估计方法
def desity_at_quantile(
    x: List[float], 
    tau: float, 
    kernel: Callable[float, float] = lambda u: 3. / 4 * (1-u**2) * np.where(np.abs(u) <= 1, 1, 0), 
    bandwidth: Callable[[float, float], float] = hall_sheather
):
    assert len(x) > 0
    assert tau >= 0.0 and tau <= 1.0
    
    x = np.array(x)
    nobs = len(x)
    e = x - np.quantile(x, tau)

    iqre = stats.scoreatpercentile(e, 75) - stats.scoreatpercentile(e, 25)
    h = bandwidth(nobs, tau)
    # Greene (2008, p.407) writes that Stata 6 uses this bandwidth:
    # h = 0.9 * np.std(e) / (nobs**0.2)
    # Instead, we calculate bandwidth as in Stata 12
    h = min(np.std(x), iqre / 1.34) * (norm.ppf(tau + h) - norm.ppf(tau - h))
    return 1. / (nobs * h) * np.sum(kernel(e / h))


```


```{python}
from pyqreg.utils import generate_clustered_data, rng_generator
from pyqreg import QuantReg
import numpy as np
import pyqreg
import scipy.stats as stats
from scipy.stats import norm
import numpy.typing as npt
import warnings
import scipy.stats as stats
from scipy.stats import norm
from typing import Optional

def hall_sheather(n, q, alpha=.05):
    z = norm.ppf(q)
    num = 1.5 * norm.pdf(z)**2.
    den = 2. * z**2. + 1.
    h = n**(-1. / 3) * norm.ppf(1. - alpha / 2.)**(2./3) * (num / den)**(1./3)
    return h

def estimate_cov(
    X: npt.NDArray[np.float64], 
    y: npt.NDArray[np.float64], 
    beta: npt.NDArray[np.float64],  
    tau: float = 0.5, 
    groups: Optional[npt.NDArray[np.int32]] = None,
    kappa_type: str = "silverman"
):
    assert len(X) == len(y)
    if groups is not None:
        assert len(X) == len(groups)
    assert len(beta.shape) == 1 and X.shape[1] == beta.shape[0]
    assert tau >= 0.0 and tau <= 1.0
    assert kappa_type in ['silverman', 'median']

    resid = y - X @ beta
    psi_resid = np.where(resid <= 0, tau - 1, tau) # psi_function
    if groups is not None:
        # group by groups and xe = sum(X * r), r是标量
        order = groups.argsort(kind="mergesort")
        groups = groups[order].astype(np.int32)
        g_ind = np.ones(len(groups), 'bool')
        g_ind[:-1] = groups[1:] != groups[:-1]
        X = X[order]
        y = y[order]
        xe = (X * psi_resid[:, np.newaxis]).cumsum(axis=0)[g_ind]
        xe[1:] = xe[1:] - xe[:-1]
    else:
        xe = (X * psi_resid[:, np.newaxis])
    A = xe.T @ xe

    hg = hall_sheather(n, tau)
    if kappa_type == "median":
        k = np.median(np.abs(resid))
    elif kappa_type == "silverman":
        iqre = np.quantile(resid, 0.75) - np.quantile(resid, 0.25)
        k = min(np.std(resid), iqre / 1.34)
    else:
        raise ValueError(
            "Incorrect kappa_type {}. Please choose between median and silverman".format(
                kappa_type
            )
        )
    hg = k * (norm.ppf(tau + hg) - norm.ppf(tau - hg))

    kernel = np.sqrt(np.abs(resid) < hg).astype(np.float64) / (2 * hg)

    # c^_G
    # chat_G = k * (invnormal(theta + h_nG) - invnormal(theta - h_nG))

    # # B weights
    dens = np.sqrt((np.abs(resid) < chat_G).astype(np.float64) / (2 * chat_G))


    # mask = np.where(kernel)
    # xd = X[mask] * kernel[mask][:, np.newaxis]
    xd = X * kernel[:, np.newaxis]
    B = xd.T @ xd
    B_inv = np.linalg.pinv(B)

    return B_inv @ A @ B_inv

rng = rng_generator(0)
y, X, groups = generate_clustered_data(150, 500, 15, rng)
mod = QuantReg(y, X)
tau = 0.6
res = mod.fit(tau, cov_type='cluster', cov_kwds={'groups': groups})
beta = res.params
print(f"tau: {tau}, b: {beta}, stderr: {res.bse}")
estimate_cov(X, y, beta, tau, groups)
# tau: 0.6, b: [38.2355112  -5.40712834], stderr: [1.48802353 2.39916696]

# (psi_resid == pyqreg.c.cluster_cov.psi_function(resid, tau)).all()
A_c = pyqreg.c.matrix_opaccum.matrix_opaccum(
    np.array(X, np.double, copy=True, order="F", ndmin=1), 
    groups, psi_resid, 
    len(np.unique(groups))
)
(A - A_c < 1e-6).all()

B_c = pyqreg.c.matrix_opaccum.matrix_opaccum(
    np.array(X, np.double, copy=True, order="F", ndmin=1), 
    np.arange(len(groups)).astype(np.int32), 
    dens.astype(np.float64), 
    n
)
(B - B_c < 1e-6).all()

# (B_inv - pyqreg.c.blas_lapack.lapack_cholesky_inv(B.copy()) < 1e-6).all()

# return B @ A @ B



# x1 = np.array([[1.0, 2.0], [1.3, 2.2], [0.8, 2.7]])
# # x1 = np.array([[1.0, 2.0], [1.3, 2.2]])
# g1 = np.arange(x1.shape[0], dtype = np.int32)
# p1 = np.repeat(1.0, x1.shape[0])
# pyqreg.c.matrix_opaccum.matrix_opaccum(x1, g1, p1, len(np.unique(g1)))
# array([[5.  , 5.7 ],
#        [5.7 , 6.53]])

# np.dot(x1.T, x1)
# np.dot(x1, x1.T)


# A1 = pyqreg.c.matrix_opaccum.matrix_opaccum(X, np.arange(X.shape[0], dtype = np.int32), psi_resid, X.shape[0])
# xe = (X * psi_resid[:, np.newaxis])
# np.dot(xe.T, xe) / A1
# array([[13153.6, 10993. ],
#        [10993. , 13320.6]])
# (Xeg[:, 0] * Xeg[:, 0]).sum()

# (Xeg[:, 0] * Xeg[:, 0]).sum() + (Xeg[:, 1] * Xeg[:, 0]).sum()

# np.dot(np.array([1, 2]).T, np.array([1, 2]))

# np.apply_along_axis(lambda x: np.dot(x.T, x), -1, Xeg)  

# arr = np.array([1.2, 2])[:, np.newaxis]
# np.dot(arr, arr.T)

# Compute residuals
resid = self.y - self.X @ beta
def cluster_cov(X, y, groups, beta, q, kappa_type="silverman"):
    
    theta = q
    n = len(self.X)

    sort_args = groups.argsort(kind="mergesort")
    self.X = self.X[sort_args]
    self.y = self.y[sort_args]
    groups = groups[sort_args]

    self.X = np.array(self.X, np.double, copy=False, order="F", ndmin=1)
    self.y = np.array(self.y, np.double, copy=False, order="F", ndmin=1)
    groups = np.array(groups, np.int32, copy=False, order="F", ndmin=1)

    G = len(np.unique(groups))

    # Compute residuals
    resid = self.y - self.X @ beta

    # Compute A

    # psi
    psi_resid = psi_function(resid, theta)

    A = matrix_opaccum(self.X, groups, psi_resid, G)

    # Compute B

    # fmt: off
    # h_nG
    h_nG = (invnormal(0.975)**(2/3)) * \
    ((1.5 * ((normalden(invnormal(theta)))**2) / (2 * ((invnormal(theta))**2) + 1))**(1/3)) * \
    (n)**(-1/3)
    # fmt: on

    # kappa
    if kappa_type == "median":
        k = np.median(np.abs(resid))
    elif kappa_type == "silverman":
        k = min(
            np.std(resid),
            (np.percentile(resid, 75) - np.percentile(resid, 25)) / 1.34,
        )
    else:
        raise ValueError(
            "Incorrect kappa_type {}. Please choose between median and silverman".format(
                kappa_type
            )
        )

    # c^_G
    chat_G = k * (invnormal(theta + h_nG) - invnormal(theta - h_nG))

    # B weights
    dens = np.sqrt((np.abs(resid) < chat_G).astype(np.float64) / (2 * chat_G))
    _groups = np.arange(len(groups)).astype(np.int32)

    B = matrix_opaccum(self.X, _groups, dens, n)

    # Compute Binv A Binv
    B = np.array(B, np.double, copy=False, order="F", ndmin=1)
    lapack_cholesky_inv(B)

    return B @ A @ B


```

