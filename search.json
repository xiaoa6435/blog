[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/quantile-ab-test/quantile-regression.html",
    "href": "posts/quantile-ab-test/quantile-regression.html",
    "title": "分位数回归的原理、代码实现和聚类标准误",
    "section": "",
    "text": "分位数回归(quantile regression)可以回答实验操作/处理对指标分布的影响，一个非常经典的例子是，教育培训对收入的影响，在这里，我们不仅关心平均收入，还关心收入的平等，比如收入的中位数等。下面我们简要概述分位数回归的原理，以及相关代码实现。\n分位数回归的核心问题是估计条件分位数\\(Q_{\\tau}(Y_i|X_i) = F_y^-1(\\tau|X_i)\\)：对连续变量Y，在给定一组协变量X下，预测其第\\(tau\\)分位数。它也是以下问题的解:\n\\[ Q_{\\tau}(Y_i|X_i) = \\mathop{\\arg\\min}\\limits_{q(X)} E[\\rho_\\tau(Y - q(X_i))] \\]\n记\\(\\rho_\\tau(u) = (\\tau - \\mathbb{1}(u \\leq 0)) * u, u = Y - q(X_i)\\)，这个公式也是分位数回归的损失函数，我们可以在xgboost、lightgbm、pytorch中自定义损失函数，实现非线性/深度分位数回归模型，但这里我们只考虑线性的情况，对线性分位数回归，_{}满足以下条件\n\\[ \\beta_{\\tau} = \\mathop{\\arg\\min}\\limits_{q(X)} E[\\rho_\\tau(Y - X\\beta)] \\]\n线性规划、内点法、迭代加权最小二乘法(IRLS: Iteratively reweighted least squares)都可以估计\\(\\beta_{\\tau}\\), R的quantreg和pyqreg采用了内点法, statsmodels的quantile_regression采用IRLS。IRLS的实现比较简单，下面是示例的代码。\n/Users/zhangzhenhao/Library/Python/3.7/bin\nimport numpy as np\nimport numpy.typing as npt\nimport warnings\nimport scipy.stats as stats\nfrom scipy.stats import norm\n\n# from numba import jit\n# @jit(nopython=True)\ndef quantreg_by_irls(\n    X: npt.NDArray[np.float64], \n    y: npt.NDArray[np.float64],\n    q: float = 0.5,\n    max_iter: int = 1000, \n    p_tol: float = 1e-6,\n    r_tol: float = 1e-6\n):\n    beta = np.ones(X.shape[1])\n    xstar = X\n    n_iter = 0\n    while n_iter &lt; max_iter:\n        n_iter += 1\n        beta0 = beta\n        xtx = np.dot(xstar.T, X)\n        xty = np.dot(xstar.T, y)\n        beta = np.dot(np.linalg.pinv(xtx), xty)\n        # 收敛条件\n        if np.max(np.abs(beta - beta0)) &lt;= p_tol:\n            break\n\n        resid = y - np.dot(X, beta)\n        # avoid dividing by zero\n        mask = np.abs(resid) &lt; r_tol\n        resid[mask] = ((resid[mask] &gt;= 0) * 2 - 1) * r_tol\n\n        # 重新加权，check function是np.where(resid &lt; 0, -(1 - q) * resid, q * resid)\n        resid = np.where(resid &lt; 0, -q * resid, (1 - q) * resid)\n        xstar = X / resid[:, np.newaxis]\n        \n    if n_iter == max_iter:\n        warnings.warn(f\"Maximum number of iterations ({max_iter}) reached.\")\n\n    return beta\n\n(n0, mu0, sd0) = (300000, 0.0, 1.0)\n(n1, mu1, sd1) = (310000, 0.2, 0.8)\ntau = 0.7\ntq = norm.pdf(tau, mu1, sd1) - norm.pdf(tau, mu0, sd0)  # 真实的qte\ny0 = np.random.normal(mu0, sd0, n0)\ny1 = np.random.normal(mu1, sd1, n1) \n\nX = np.array([\n  np.ones(n0 + n1),\n  np.repeat([0, 1], [n0, n1])\n]).T\ny = np.concatenate([y0, y1])\n\nbeta = quantreg_by_irls(X, y, tau)\n(q0, q1) = np.quantile(y0, tau), np.quantile(y1, tau) \nprint(f\"beta:{beta}, y0的tau分位数: {q0}, y1的tau分位数 - y0的tau分位数: {q1 - q0}\")\n\n# %timeit quantreg_by_irls_pure(X, y, tau)\n# 4.99 s ± 459 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n# %timeit quantreg_by_irls_fast(X, y, tau)\n# 3.55 s ± 95.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nbeta:[0.52324171 0.09725338], y0的tau分位数: 0.5232449700516071, y1的tau分位数 - y0的tau分位数: 0.09726847426621332\n我们模拟了\\(y \\sim 1 + D\\)，其中D是0，1分组变量，在这种情况下，beta实际就是\\([Q_{y_0}(\\tau), Q_{y_1}(\\tau) - Q_{y_0}(\\tau)]\\)."
  },
  {
    "objectID": "posts/quantile-ab-test/quantile-regression.html#协方差矩阵估计",
    "href": "posts/quantile-ab-test/quantile-regression.html#协方差矩阵估计",
    "title": "分位数回归的原理、代码实现和聚类标准误",
    "section": "协方差矩阵估计",
    "text": "协方差矩阵估计\n在残差满足i.i.d假设下，\\(\\beta\\)的协方差矩阵渐进于以下公式：\n\\[ \\sqrt{n}(\\hat{\\beta}(\\tau) - {\\beta}(\\tau)) \\sim \\frac{\\tau * (1 - \\tau)}{f_{\\mu_{\\tau}}(0|X_i)}(X^{'}X)^{-1} \\],\n如果有异方差的问题\n\\[ \\sqrt{n}(\\hat{\\beta}(\\tau) - {\\beta}(\\tau)) \\sim \\frac{\\tau * (1 - \\tau)}{f_{\\mu_{\\tau}}(0|X_i)}(X^{'}X)^{-1} \\]\n\nfrom typing import List, Callable\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import norm\n\ndef hall_sheather(n, tau, alpha=.05):\n    z = norm.ppf(tau)\n    num = 1.5 * norm.pdf(z)**2.\n    den = 2. * z**2. + 1.\n    h = n**(-1. / 3) * norm.ppf(1. - alpha / 2.)**(2./3) * (num / den)**(1./3)\n    return h\n\n# stat.model中quantreg的默认估计方法\ndef desity_at_quantile(\n    x: List[float], \n    tau: float, \n    kernel: Callable[float, float] = lambda u: 3. / 4 * (1-u**2) * np.where(np.abs(u) &lt;= 1, 1, 0), \n    bandwidth: Callable[[float, float], float] = hall_sheather\n):\n    assert len(x) &gt; 0\n    assert tau &gt;= 0.0 and tau &lt;= 1.0\n    \n    x = np.array(x)\n    nobs = len(x)\n    e = x - np.quantile(x, tau)\n\n    iqre = stats.scoreatpercentile(e, 75) - stats.scoreatpercentile(e, 25)\n    h = bandwidth(nobs, tau)\n    # Greene (2008, p.407) writes that Stata 6 uses this bandwidth:\n    # h = 0.9 * np.std(e) / (nobs**0.2)\n    # Instead, we calculate bandwidth as in Stata 12\n    h = min(np.std(x), iqre / 1.34) * (norm.ppf(tau + h) - norm.ppf(tau - h))\n    return 1. / (nobs * h) * np.sum(kernel(e / h))\n\n\nfrom pyqreg.utils import generate_clustered_data, rng_generator\nfrom pyqreg import QuantReg\nimport numpy as np\nimport pyqreg\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport numpy.typing as npt\nimport warnings\nfrom typing import Optional\n\ndef hall_sheather(n, q, alpha=.05):\n    z = norm.ppf(q)\n    num = 1.5 * norm.pdf(z)**2.\n    den = 2. * z**2. + 1.\n    h = n**(-1. / 3) * norm.ppf(1. - alpha / 2.)**(2./3) * (num / den)**(1./3)\n    return h\n\ndef estimate_cov(\n    X: npt.NDArray[np.float64], \n    y: npt.NDArray[np.float64], \n    beta: npt.NDArray[np.float64],  \n    tau: float = 0.5, \n    groups: Optional[npt.NDArray[np.int32]] = None,\n    kappa_type: str = \"silverman\"\n):\n    assert X.shape[0] == y.shape[0]\n    if groups is not None:\n        assert X.shape[0] == groups.shape[0]\n    assert len(beta.shape) == 1 and X.shape[1] == beta.shape[0]\n    assert tau &gt;= 0.0 and tau &lt;= 1.0\n    assert kappa_type in ['silverman', 'median']\n\n    resid = y - X @ beta\n    psi_resid = np.where(resid &lt;= 0, tau - 1, tau) # psi_function\n    if groups is not None:\n        # group by cluster and xe = sum(X * r), r是标量，代表残差\n        # 参考https://stackoverflow.com/questions/4373631/sum-array-by-number-in-numpy中Bi_Rico and Sven的回答\n        order = groups.argsort()\n        groups = groups[order].astype(np.int32)\n        g_ind = np.ones(groups.shape[0], 'bool')\n        g_ind[:-1] = groups[1:] != groups[:-1]\n        X = X[order]\n        y = y[order]\n        xe = (X * psi_resid[:, np.newaxis]).cumsum(axis=0)[g_ind]\n        xe[1:] = xe[1:] - xe[:-1]\n    else:\n        xe = (X * psi_resid[:, np.newaxis])\n    A = xe.T @ xe\n\n    n = X.shape[0]\n    hg = hall_sheather(n, tau)\n    if kappa_type == \"median\":\n        k = np.median(np.abs(resid))\n    elif kappa_type == \"silverman\":\n        iqre = np.quantile(resid, 0.75) - np.quantile(resid, 0.25)\n        k = min(np.std(resid), iqre / 1.34)\n    else:\n        raise ValueError(\n            \"Incorrect kappa_type {}. Please choose between median and silverman\".format(\n                kappa_type\n            )\n        )\n    hg = k * (norm.ppf(tau + hg) - norm.ppf(tau - hg))\n    kernel = np.sqrt((np.abs(resid) &lt; hg) / (2.0 * hg))\n    xd = X * kernel[:, np.newaxis]\n    B = xd.T @ xd\n    B_inv = np.linalg.pinv(B)\n    return B_inv @ A @ B_inv\n\nrng = rng_generator(0)\ny, X, groups = generate_clustered_data(150, 500, 15, rng)\nmod = QuantReg(y, X)\ntau = 0.6\nres = mod.fit(tau, cov_type='cluster', cov_kwds={'groups': groups})\nbeta = res.params\nprint(f\"tau: {tau}, b: {beta}, stderr: {res.bse}\")\n# tau: 0.6, b: [38.2355112  -5.40712834], stderr: [1.48802353 2.39916696]\nvar = estimate_cov(X, y, beta, tau, groups)\nnp.isclose(np.abs(np.sqrt(np.diag(var))), res.bse)\n\n\n# (psi_resid == pyqreg.c.cluster_cov.psi_function(resid, tau)).all()\n# A_c = pyqreg.c.matrix_opaccum.matrix_opaccum(\n#     np.array(X, np.double, copy=True, order=\"F\", ndmin=1), \n#     groups, psi_resid, \n#     len(np.unique(groups))\n# )\n# (A - A_c &lt; 1e-6).all()\n\n# B_c = pyqreg.c.matrix_opaccum.matrix_opaccum(\n#     np.array(X, np.double, copy=True, order=\"F\", ndmin=1), \n#     np.arange(len(groups)).astype(np.int32), \n#     dens.astype(np.float64), \n#     n\n# )\n# (B - B_c &lt; 1e-6).all()\n# (B_inv - pyqreg.c.blas_lapack.lapack_cholesky_inv(B.copy()) &lt; 1e-6).all()\n# from pyqreg.c.stats import invnormal, normalden\n\ntau: 0.6, b: [38.2355112  -5.40712834], stderr: [1.48802353 2.39916696]\n\n\narray([ True,  True])"
  },
  {
    "objectID": "posts/switchback-experiment/index.html",
    "href": "posts/switchback-experiment/index.html",
    "title": "时间片轮转实验的设计和分析",
    "section": "",
    "text": "时间片轮转实验(switchback experiment)常见于双边/多边市场中，比如滴滴、uber等出行市场，或者美团外卖、饿了么、DoorDash等外卖平台的派单算法/策略实验，在这类实验中，司机和司机之间，乘客和乘客之间，本次派单和下次派单，都有无法忽略的相互影响。另一个常见的场景是价格策略实验：由于政策原因，同一时间在同一个城市，不能出现不同的价格。\n一个典型的时间片轮转实验大概是这样的:\n从实验设计的角度来看，时间片轮转实验主要关注以下三个问题：\n下面的分析主要参照DASEDesign and Analysis of Switchback Experiments"
  },
  {
    "objectID": "posts/switchback-experiment/index.html#dase的论文",
    "href": "posts/switchback-experiment/index.html#dase的论文",
    "title": "时间片轮转实验的设计和分析",
    "section": "DASE的论文",
    "text": "DASE的论文\n基于潜在结果框架，DASE给出了时间片轮转实验的最优设计、非参数的实验处理效应及相应的标准误、p值的估计。\n基于DASE，时间片轮转实验需要预先知道三个参数：\n\n实验持续长度：比如持续7天\n时间片的粒度：这个参数也是需要预先指定的，比如30min\n延滞效应(carryover effect)的阶数m: 当前的时间片对后续时间片的影响最多持续到第m个，m = 0意味着时间片之间彼此独立，这个参数也是根据业务经验指定的\n\n\n最优设计\n@bojinov2023design 的Theorem 3给出了时间片轮转实验的最优设计（注：原文的下标是从1开始，这里改成了从0开始）：\n\n当m = 0是，\\(T^* = {0, 1, 2, 3, 4, ...., T - 1}\\)，这种情况就是标准的随机分组实验\n当m &gt; 0且T &gt;= 4m (存在T = n * m, 其中 n &gt;= 4)时，\\(T^* = {0, 2m, 3m, ...., (n - 2)m, T - 1}\\)\n\n不属于以上两种情况的实验（T很小，或者m很大）讨论价值较小，后续我们重点关注情况2\n这里的\\(T^*\\)(randomization_points)是随机点的集合，举个具体的例子：\n\n假设time_horizon = 12(有12个时间片)，m = 2, 这个时候满足第二个条件\n最优设计给出的随机点是[0, 4, 6, 8]\n抛4次硬币(有4个随机点)，[0, 4)按第0次硬币的结果进行处理，[4, 6)都是第1次结果…\n不妨假设4次硬币的结果分别是[T, C, T, T], 那12个时间片的实验分组分别是[T, T, T, T, C, C, T, T, T, T, T, T]\n\n这个设计和朴素设计，即把m个连续的时间片合成一个大的时间片，再做简单随机，随机点是[0, (m + 1), 2 * (m + 1), ….]有冲突。作者证明了，基于极小极大化原则，最优方案有最小的标准误\n最优设计和朴素设计的代码如下\n\nfrom typing import List\nimport random\n\ndef generate_opt_design(time_horizon: int, m: int) -&gt; List[int]:\n    assert time_horizon &gt; 0\n    assert m &gt; 0\n\n    randomization_points = [0] + list(range(2 * m, time_horizon - 2 * m + 1, m))\n    return randomization_points\n\n\ndef generate_naive_design(time_horizon: int, m: int) -&gt; List[int]:\n    assert time_horizon &gt; 0\n    assert m &gt;= 0\n\n    return list(range(0, time_horizon, m + 1))\n\n\ndef generate_assignment_path(randomization_points: List[int], time_horizon: int, m: int, p: float = 0.5) -&gt; (List[int], List[int]):\n    assert len(randomization_points) &gt; 0\n\n    assert time_horizon &gt; 0\n    assert m &gt;= 0\n    assert p &gt; 0.0 and p &lt; 1.0\n\n    randomization_points = list(randomization_points) + [time_horizon]\n    assignment_paths = []\n    for i in range(1, len(randomization_points)):\n        assignment = 1 if random.uniform(0.0, 1.0) &lt; p else 0\n        for j in range(randomization_points[i - 1], randomization_points[i]):\n            assignment_paths.append(assignment)\n    return assignment_paths\n\n# generate_opt_design(12, 2)\n# generate_naive_design(12, 2)\n\n\n\n实验处理效应估计\n在@bojinov2023design 的2.4, 基于Horvitz-Thompson估计，m阶(原文中的lag-p effect)的处理效应的公式如下:\n\\[\n\\hat{\\tau_p}(\\eta_T, \\omega_{1:T}, Y) = \\frac{1}{T - m}(Y_{t}^{obs} * \\sum_{t = m}^{T - 1}( \\frac{\\mathbb{1}{(W_{t - m: t} = \\mathbb{1}_{m + 1})}}{Pr(W_{t - m: t} = \\mathbb{1}_{m + 1})} - Y_{t}^{obs} * \\frac{\\mathbb{1}{(W_{t - m: t} = \\mathbb{0}_{m + 1})}}{Pr(W_{t - m: t} = \\mathbb{0}_{m + 1})}))\n\\]\n\n其中\\(\\tau_p\\)是随机点，上文的randomization_points\n\\(\\omega_{1:T}\\)是各个时间片的分组，上文的assignment_paths\n这个估计是非参的：它并没有假设m阶的具体的函数形式\n\n看起来有点复杂，我们从一个具体的例子来看, 假设有6个时间片, m = 1, 实验组占比p = 0.4（注：以下所有设计到第x个的地方都是从0开始）\n\n随机点randomization_points = [0, 3, 5], 抛三次硬币的结果分别是[1, 0, 0], 6个时间片的分组分别是[1, 1, 1, 0, 0, 0]\n观察到各个时间片的指标数y是[1.2, 2.1, 3.0, 2.0, 0.5, 1.6]\n\\(\\tau\\)的估计只考虑从当前时间片向前推m个窗口连续是1/0的窗口, 所有6个窗口分别是[nan, 1, 1, nan, 0, 0], 因为第二位和第三位(从0开始)不全是0，或者1，所有第三个是nan\n6个窗口对应的 \\(Pr(W_{t - m: t}\\) 分别是[nan, 0.4, 0.4, nan, 0.6, 0.36], 第5个窗口是0.36，因为它需要第一次抛硬币的结果和第二次都是对照组\n\\(\\tau = \\frac{1}{6 - 1} * (2.1 / 0.4 + 3.0 / 0.4 - 0.5 / 0.4 - 1.6 / 0.36) = 1.41\\), nan对应的窗口对应的数据会直接抛弃\n\n\nfrom typing import List\n\ndef estimate_tau(\n    randomization_points: List[int],\n    assignment_path: List[int],\n    potential_outcome_path: List[float],\n    m: int,\n    p: float = 0.5\n) -&gt; (float, float, float, float, float):\n\n    assert len(randomization_points) &lt;= len(assignment_path)\n    assert len(assignment_path) == len(potential_outcome_path)\n\n    time_horizon = len(assignment_path)\n    randomization_points_withend = randomization_points + [time_horizon]\n    randomization_ids = []  # 每个时间片对应的随机数的id\n    for i in range(0, len(randomization_points)):\n        for j in range(randomization_points_withend[i], randomization_points_withend[i + 1]):\n            randomization_ids.append(i)\n\n    y_trt, y_ctl, n_trt, n_ctl = 0.0, 0.0, 0.0, 0.0\n    for t in range(m, time_horizon):\n        (beg, end) = (t - m, t + 1)\n        wsum = sum(assignment_path[beg:end])\n        # 窗口内涉及到的随机数id的个数\n        randomization_cnt = len(set(randomization_ids[beg:end])) \n        if wsum == (m + 1):\n            ps = p ** randomization_cnt\n            y_trt += potential_outcome_path[t] / ps\n            n_trt += 1\n        elif wsum == 0:\n            ps = (1.0 - p) ** randomization_cnt\n            y_ctl += potential_outcome_path[t] / ps\n            n_ctl += 1\n        else:\n            pass\n    n = (len(assignment_path) - m)\n    tau = y_trt / n - y_ctl / n\n\n    return (tau, y_trt / n, y_ctl / n, n_trt, n_ctl)\n\n# randomization_points = [0, 3, 5]\n# assignment_path = [1, 1, 1, 0, 0, 0]\n# potential_outcome_path = [1.2, 2.1, 3.0, 2.0, 0.5, 1.6]\n# m = 1\n# p = 0.4\n# estimate_tau(randomization_points, assignment_path, potential_outcome_path, m, p)\n\n在 @bojinov2023design 的4.2，作者给出了在最优设计下的两种估计方差的公式。这两个公式都比较保守（比实际方差大），相应的代码实现如下\n\nfrom typing import List\n\ndef generate_ub_variance_opt_design(\n    randomization_points: List[int],\n    assignment_path: List[int],\n    potential_outcome_path: List[float],\n    m: int,\n    upper_bound_type: int = 2\n) -&gt; float:\n\n    num_randomizations = len(randomization_points)\n    observed_chunks = [sum(potential_outcome_path[(i * m + 2):((i + 1) * m + 2)]) for i in range(num_randomizations + 1)]\n\n    if upper_bound_type == 1:\n        variance = 6 * observed_chunks[0] ** 2 + 6 * observed_chunks[num_randomizations] ** 2\n        for k in range(1, num_randomizations):\n            if assignment_path[k * m] == assignment_path[(k + 1) * m]:\n                variance += 24.0 * observed_chunks[k] ** 2\n\n        for k in range(num_randomizations):\n            if (assignment_path[k * m] == assignment_path[(k + 1) * m] and\n                    assignment_path[(k + 2) * m] == assignment_path[(k + 1) * m]):\n                variance += 16.0 * observed_chunks[k] * observed_chunks[k + 1]\n                \n    elif upper_bound_type == 2:\n        variance = 8 * observed_chunks[0] ** 2 + 8 * observed_chunks[num_randomizations] ** 2\n        for k in range(1, num_randomizations):\n            if assignment_path[k * m] == assignment_path[(k + 1) * m]:\n                variance += 32 * observed_chunks[k] ** 2\n    else:\n        raise ValueError(\"upper_bound_type in (1, 2)\")\n\n    return variance / ((time_horizon - m) ** 2)\n\n有了处理效应和相应的方差，可以构建置信区间和相应的p值等。\n另外，也可以根据fisher检验的逻辑计算p值。这个逻辑比较直接，下面直接上代码\n\nfrom typing import List\n\ndef fisher_test_switchback(\n    randomization_points: List[int], \n    assignment_path: List[int],\n    potential_outcome_path: List[float],\n    m: int,\n    permutations: int = 10000,\n    alternative: str = 'two-sided'\n) -&gt; float:\n    assert m &gt;= 0\n    assert p &gt; 0.0 and p &lt; 1.0\n    assert permutations &gt; 0\n    assert alternative in ['two-sided', 'greater', 'less']\n\n    randomization_points = generate_opt_design(time_horizon, m)\n    realized_tau = estimate_tau(randomization_points, assignment_path, potential_outcome_path, m, p)[0]\n    p_value = 0.0\n\n    for i in range(permutations):\n        sim_assignment_path = generate_assignment_path(randomization_points, time_horizon, m, p)\n        sim_tau = estimate_tau(randomization_points, sim_assignment_path, potential_outcome_path, m, p)[0]\n        if alternative == 'two-sided' and abs(sim_tau) &gt; abs(realized_tau):\n            p_value += 1.0\n        elif alternative == 'greater' and sim_tau &gt; realized_tau:\n            p_value += 1.0\n        elif alternative == 'less' and sim_tau &lt; realized_tau:\n            p_value += 1.0\n    return p_value / permutations\n\n把处理效应和方差估计整合起来，可以给出最终的结果\n\nimport scipy\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass SwitchbackTestRes:\n    m: int\n    time_horizon: int\n    tau: float\n    treat_mean: float\n    control_mean: float\n    treat_cnt: int\n    control_cnt: int\n    stderr: float = float('nan')\n    p_value: float = float('nan')\n    p: float = 0.5,\n    alternative: str = 'two-sided'\n    permutations: int = 0\n    confidence_level: float = 0.95,\n    upper_bound_type: int = 2\n\n    def __repr__(self) -&gt; str:\n        smy = f\"\"\"\n        time_horizon: {self.time_horizon}, carryover order: {self.m}, treat prob: {self.p}\n        tau: {self.tau}, sample treat mean: {self.treat_mean}, sample control_mean: {self.control_mean},\n        valid treat window: {self.treat_cnt}, valid control window: {self.control_cnt}\n        alternative hypothesis: true difference in means is {'not equal to' if self.alternative == 'two-sided' else self.alternative + ' than'} 0\n        \"\"\"\n\n        if self.permutations &gt; 0:\n            smy += f\"\"\"\n            use fisher test, permutations: {self.permutations}\n            p value: {self.p_value}\n            \"\"\"\n        else:\n            q = scipy.stats.norm.cdf(self.tau / self.stderr)\n            if self.alternative == 'two-sided':\n                self.p_value = 2.0 * (1.0 - q)\n            elif self.alternative == 'greater':\n                self.p_value = 1.0 - q\n            elif self.alternative == 'less':\n                self.p_value = q\n            smy += f\"\"\"\n            use neyman test, std err: {self.stderr}\n            p value: {self.p_value}\n            \"\"\"\n\n            d = -scipy.stats.norm.ppf((1.0 - self.confidence_level) / 2.0)\n            smy += f\"\"\"\n            upper_bound_type: {self.upper_bound_type}\n            {self.confidence_level * 100} percent confidence interval: [{self.tau - d * self.stderr}, {self.tau + d * self.stderr}]\n            \"\"\"\n        return \"\\n\".join([line.lstrip() for line in smy.split(\"\\n\")])\n    \n# SwitchbackTestRes(2, 120, 3.0, 8.4, 5.4, 20, 18, 1.9)\n\ndef switchback_test(\n    randomization_points: List[int],\n    assignment_path: List[int],\n    potential_outcome_path: List[float],\n    m: int,\n    p: float = 0.5,\n    alternative: str = 'two-sided',\n    permutations: int = 0,\n    confidence_level: float = 0.95,\n    upper_bound_type: int = 2\n):\n\n    assert set(assignment_path) == set([0, 1])\n    assert m &gt;= 0\n    assert p &gt; 0.0 and p &lt; 1.0\n    assert alternative in ['two-sided', 'greater', 'less']\n    assert permutations &gt;= 0\n    assert confidence_level &gt; 0.0 and confidence_level &lt; 1.0\n    assert upper_bound_type in [1, 2]\n\n    (tau, m_trt, m_ctl, n_trt, n_ctl) = estimate_tau(randomization_points, assignment_path, potential_outcome_path, m, p)\n    \n    if permutations &lt;= 0:\n        var = generate_ub_variance_opt_design(randomization_points, assignment_path, potential_outcome_path, m, upper_bound_type)\n        stderr = math.sqrt(var)\n        p_value = float('nan')\n    else: \n        stderr = float('nan')\n        p_value = fisher_test_switchback(randomization_points, assignment_path, potential_outcome_path, m, permutations, alternative)\n    \n    time_horizon = len(assignment_path) \n    return SwitchbackTestRes(\n      m, time_horizon, tau, m_trt, m_ctl, n_trt, n_ctl, stderr, p_value, p,\n      alternative, permutations, confidence_level, upper_bound_type\n    )\n\n# time_horizon = 120\n# m = 2\n# p = 0.5\n# randomization_points = generate_opt_design(time_horizon, m)\n# assignment_path = generate_assignment_path(randomization_points, time_horizon, m)\n# potential_outcome_path = generate_outcome(assignment_path, [1.0, 1.0, 1.0])\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p)\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, permutations = 1000)\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'two-sided', confidence_level = 0.90, upper_bound_type = 1)\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'less')\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'greater'\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'two-sided', permutations = 1000)\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'less', permutations = 1000)\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'greater', permutations = 1000)"
  },
  {
    "objectID": "posts/switchback-experiment/index.html#在线demo",
    "href": "posts/switchback-experiment/index.html#在线demo",
    "title": "时间片轮转实验的设计和分析",
    "section": "在线demo",
    "text": "在线demo\n#| components: [editor, cell]\n\nimport random\nimport math\nimport scipy\nfrom dataclasses import dataclass\nfrom typing import List\n\n\ndef generate_opt_design(time_horizon: int, m: int) -&gt; List[int]:\n    assert time_horizon &gt; 0\n    assert m &gt; 0\n\n    randomization_points = [0] + list(range(2 * m, time_horizon - 2 * m + 1, m))\n    return randomization_points\n\n\ndef generate_naive_design(time_horizon: int, m: int) -&gt; List[int]:\n    assert time_horizon &gt; 0\n    assert m &gt;= 0\n\n    return list(range(0, time_horizon, m + 1))\n\n\ndef generate_assignment_path(randomization_points: List[int], time_horizon: int, m: int, p: float = 0.5) -&gt; (List[int], List[int]):\n    assert len(randomization_points) &gt; 0\n\n    assert time_horizon &gt; 0\n    assert m &gt;= 0\n    assert p &gt; 0.0 and p &lt; 1.0\n\n    randomization_points = list(randomization_points) + [time_horizon]\n    assignment_paths = []\n    for i in range(1, len(randomization_points)):\n        assignment = 1 if random.uniform(0.0, 1.0) &lt; p else 0\n        for j in range(randomization_points[i - 1], randomization_points[i]):\n            assignment_paths.append(assignment)\n    return assignment_paths\n\ndef estimate_tau(\n    randomization_points: List[int],\n    assignment_path: List[int],\n    potential_outcome_path: List[float],\n    m: int,\n    p: float = 0.5\n) -&gt; (float, float, float, float, float):\n\n    assert len(randomization_points) &lt;= len(assignment_path)\n    assert len(assignment_path) == len(potential_outcome_path)\n\n    time_horizon = len(assignment_path)\n    randomization_points_withend = randomization_points + [time_horizon]\n    randomization_ids = []  # 每个时间片对应的随机数的id\n    for i in range(0, len(randomization_points)):\n        for j in range(randomization_points_withend[i], randomization_points_withend[i + 1]):\n            randomization_ids.append(i)\n\n    y_trt, y_ctl, n_trt, n_ctl = 0.0, 0.0, 0.0, 0.0\n    for t in range(m, time_horizon):\n        (beg, end) = (t - m, t + 1)\n        wsum = sum(assignment_path[beg:end])\n        # 窗口内涉及到的随机数id的个数\n        randomization_cnt = len(set(randomization_ids[beg:end])) \n        if wsum == (m + 1):\n            ps = p ** randomization_cnt\n            y_trt += potential_outcome_path[t] / ps\n            n_trt += 1\n        elif wsum == 0:\n            ps = (1.0 - p) ** randomization_cnt\n            y_ctl += potential_outcome_path[t] / ps\n            n_ctl += 1\n        else:\n            pass\n    n = (len(assignment_path) - m)\n    tau = y_trt / n - y_ctl / n\n    return (tau, y_trt / n, y_ctl / n, n_trt, n_ctl)\n\ndef fisher_test_switchback(\n    randomization_points: List[int], \n    assignment_path: List[int],\n    potential_outcome_path: List[float],\n    m: int,\n    permutations: int = 10000,\n    alternative: str = 'two-sided'\n) -&gt; float:\n    assert m &gt;= 0\n    assert p &gt; 0.0 and p &lt; 1.0\n    assert permutations &gt; 0\n    assert alternative in ['two-sided', 'greater', 'less']\n\n    randomization_points = generate_opt_design(time_horizon, m)\n    realized_tau = estimate_tau(randomization_points, assignment_path, potential_outcome_path, m, p)[0]\n    p_value = 0.0\n\n    for i in range(permutations):\n        sim_assignment_path = generate_assignment_path(randomization_points, time_horizon, m, p)\n        sim_tau = estimate_tau(randomization_points, sim_assignment_path, potential_outcome_path, m, p)[0]\n        if alternative == 'two-sided' and abs(sim_tau) &gt; abs(realized_tau):\n            p_value += 1.0\n        elif alternative == 'greater' and sim_tau &gt; realized_tau:\n            p_value += 1.0\n        elif alternative == 'less' and sim_tau &lt; realized_tau:\n            p_value += 1.0\n    return p_value / permutations\n\ndef generate_ub_variance_opt_design(\n    randomization_points: List[int],\n    assignment_path: List[int],\n    potential_outcome_path: List[float],\n    m: int,\n    upper_bound_type: int = 2\n) -&gt; float:\n\n    num_randomizations = len(randomization_points)\n    observed_chunks = [sum(potential_outcome_path[(i * m + 2):((i + 1) * m + 2)]) for i in range(num_randomizations + 1)]\n\n    if upper_bound_type == 1:\n        variance = 6 * observed_chunks[0] ** 2 + 6 * observed_chunks[num_randomizations] ** 2\n        for k in range(1, num_randomizations):\n            if assignment_path[k * m] == assignment_path[(k + 1) * m]:\n                variance += 24.0 * observed_chunks[k] ** 2\n\n        for k in range(num_randomizations):\n            if (assignment_path[k * m] == assignment_path[(k + 1) * m] and\n                    assignment_path[(k + 2) * m] == assignment_path[(k + 1) * m]):\n                variance += 16.0 * observed_chunks[k] * observed_chunks[k + 1]\n                \n    elif upper_bound_type == 2:\n        variance = 8 * observed_chunks[0] ** 2 + 8 * observed_chunks[num_randomizations] ** 2\n        for k in range(1, num_randomizations):\n            if assignment_path[k * m] == assignment_path[(k + 1) * m]:\n                variance += 32 * observed_chunks[k] ** 2\n    else:\n        raise ValueError(\"upper_bound_type in (1, 2)\")\n\n    return variance / ((time_horizon - m) ** 2)\n\ndef fisher_test_switchback(\n    randomization_points: List[int], \n    assignment_path: List[int],\n    potential_outcome_path: List[float],\n    m: int,\n    permutations: int = 10000,\n    alternative: str = 'two-sided'\n) -&gt; float:\n    assert m &gt;= 0\n    assert p &gt; 0.0 and p &lt; 1.0\n    assert permutations &gt; 0\n    assert alternative in ['two-sided', 'greater', 'less']\n\n    randomization_points = generate_opt_design(time_horizon, m)\n    realized_tau = estimate_tau(randomization_points, assignment_path, potential_outcome_path, m, p)[0]\n    p_value = 0.0\n\n    for i in range(permutations):\n        sim_assignment_path = generate_assignment_path(randomization_points, time_horizon, m, p)\n        sim_tau = estimate_tau(randomization_points, sim_assignment_path, potential_outcome_path, m, p)[0]\n        if alternative == 'two-sided' and abs(sim_tau) &gt; abs(realized_tau):\n            p_value += 1.0\n        elif alternative == 'greater' and sim_tau &gt; realized_tau:\n            p_value += 1.0\n        elif alternative == 'less' and sim_tau &lt; realized_tau:\n            p_value += 1.0\n    return p_value / permutations\n\n@dataclass\nclass SwitchbackTestRes:\n    m: int\n    time_horizon: int\n    tau: float\n    treat_mean: float\n    control_mean: float\n    treat_cnt: int\n    control_cnt: int\n    stderr: float = float('nan')\n    p_value: float = float('nan')\n    p: float = 0.5,\n    alternative: str = 'two-sided'\n    permutations: int = 0\n    confidence_level: float = 0.95,\n    upper_bound_type: int = 2\n\n    def __repr__(self) -&gt; str:\n        smy = f\"\"\"\n        time_horizon: {self.time_horizon}, carryover order: {self.m}, treat prob: {self.p}\n        tau: {self.tau}, sample treat mean: {self.treat_mean}, sample control_mean: {self.control_mean},\n        valid treat window: {self.treat_cnt}, valid control window: {self.control_cnt}\n        alternative hypothesis: true difference in means is {'not equal to' if self.alternative == 'two-sided' else self.alternative + ' than'} 0\n        \"\"\"\n\n        if self.permutations &gt; 0:\n            smy += f\"\"\"\n            use fisher test, permutations: {self.permutations}\n            p value: {self.p_value}\n            \"\"\"\n        else:\n            q = scipy.stats.norm.cdf(self.tau / self.stderr)\n            if self.alternative == 'two-sided':\n                self.p_value = 2.0 * (1.0 - q)\n            elif self.alternative == 'greater':\n                self.p_value = 1.0 - q\n            elif self.alternative == 'less':\n                self.p_value = q\n            smy += f\"\"\"\n            use neyman test, std err: {self.stderr}\n            p value: {self.p_value}\n            \"\"\"\n\n            d = -scipy.stats.norm.ppf((1.0 - self.confidence_level) / 2.0)\n            smy += f\"\"\"\n            upper_bound_type: {self.upper_bound_type}\n            {self.confidence_level * 100} percent confidence interval: [{self.tau - d * self.stderr}, {self.tau + d * self.stderr}]\n            \"\"\"\n        return \"\\n\".join([line.lstrip() for line in smy.split(\"\\n\")])\n\ndef switchback_test(\n    randomization_points: List[int],\n    assignment_path: List[int],\n    potential_outcome_path: List[float],\n    m: int,\n    p: float = 0.5,\n    alternative: str = 'two-sided',\n    permutations: int = 0,\n    confidence_level: float = 0.95,\n    upper_bound_type: int = 2\n):\n\n    assert set(assignment_path) == set([0, 1])\n    assert m &gt;= 0\n    assert p &gt; 0.0 and p &lt; 1.0\n    assert alternative in ['two-sided', 'greater', 'less']\n    assert permutations &gt;= 0\n    assert confidence_level &gt; 0.0 and confidence_level &lt; 1.0\n    assert upper_bound_type in [1, 2]\n\n    (tau, m_trt, m_ctl, n_trt, n_ctl) = estimate_tau(randomization_points, assignment_path, potential_outcome_path, m, p)\n    \n    if permutations &lt;= 0:\n        var = generate_ub_variance_opt_design(randomization_points, assignment_path, potential_outcome_path, m, upper_bound_type)\n        stderr = math.sqrt(var)\n        p_value = float('nan')\n    else: \n        stderr = float('nan')\n        p_value = fisher_test_switchback(randomization_points, assignment_path, potential_outcome_path, m, permutations, alternative)\n    \n    time_horizon = len(assignment_path) \n    return SwitchbackTestRes(\n      m, time_horizon, tau, m_trt, m_ctl, n_trt, n_ctl, stderr, p_value, p,\n      alternative, permutations, confidence_level, upper_bound_type\n    )\n\ndef generate_outcome(assignment_path: List[int], tau: List[float], mu: float = 0.0) -&gt; List[float]:\n\n    assert len(tau) &gt; 0, 'tau is empty'\n    assert len(assignment_path) &gt;= len(tau), 'delta is longer then assign path'\n\n    time_horizon = len(assignment_path)\n    carryover_order = len(tau)\n    ret = list()\n    for t in range(time_horizon):\n        alpht_t = math.log(t + 1)  # fixed effect associated to period t\n        epsilon = random.gauss(0.0, 1.0)  # random noise in period t\n        contemporaneous_carryover_effect = sum(\n            tau[i] * assignment_path[t - i] for i in range(min(carryover_order, t)))\n        y = mu + alpht_t + contemporaneous_carryover_effect + epsilon\n        ret.append(y)\n    return ret\n\nrandom.seed(0)\n\ntime_horizon = 120\nm = 2\np = 0.5\n\nrandomization_points = generate_opt_design(time_horizon, m) # 最优设计的随机点\nassignment_path = generate_assignment_path(randomization_points, time_horizon, m) # 每个时间片的实验分组\n# 模拟的每个时间片的指标，这里m = 2, 真实的处理效应是3.0, 具体逻辑见附-测试数据的生成\npotential_outcome_path = generate_outcome(assignment_path, [1.0, 1.0, 1.0]) \n\nswitchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, permutations = 0) # neyman asymptotic inference\nswitchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, permutations = 1000) # fisher exact inference\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'two-sided', confidence_level = 0.90, upper_bound_type = 1)\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'less')\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'greater'\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'two-sided', permutations = 1000)\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'less', permutations = 1000)\n# switchback_test(randomization_points, assignment_path, potential_outcome_path, m, p, alternative = 'greater', permutations = 1000)"
  },
  {
    "objectID": "posts/switchback-experiment/index.html#测试数据的生成逻辑",
    "href": "posts/switchback-experiment/index.html#测试数据的生成逻辑",
    "title": "时间片轮转实验的设计和分析",
    "section": "测试数据的生成逻辑",
    "text": "测试数据的生成逻辑\n测试数据根据以下公式(@bojinov2023design 的6.1的公式(15))生成\n\\[\nY_t(w_{0:t}) = \\mu + \\alpha_t + \\sum_{i = 0}^{min(m, t)}(\\delta^{(i)} * w_{t - i}) + \\epsilon_t\n\\]\n\nfrom typing import List\nimport math\n\ndef generate_outcome(assignment_path: List[int], tau: List[float], mu: float = 0.0) -&gt; List[float]:\n\n    assert len(tau) &gt; 0, 'tau is empty'\n    assert len(assignment_path) &gt;= len(tau), 'delta is longer then assign path'\n\n    time_horizon = len(assignment_path)\n    carryover_order = len(tau)\n    ret = list()\n    for t in range(time_horizon):\n        alpht_t = math.log(t + 1)  # fixed effect associated to period t\n        epsilon = random.gauss(0.0, 1.0)  # random noise in period t\n        contemporaneous_carryover_effect = sum(\n            tau[i] * assignment_path[t - i] for i in range(min(carryover_order, t)))\n        y = mu + alpht_t + contemporaneous_carryover_effect + epsilon\n        ret.append(y)\n    return ret\n\n# generate_outcome([1, 0, 1, 0], [1.0, 2.0])"
  },
  {
    "objectID": "posts/switchback-experiment/index.html#原代码",
    "href": "posts/switchback-experiment/index.html#原代码",
    "title": "时间片轮转实验的设计和分析",
    "section": "原代码",
    "text": "原代码\n@bojinov2023design 论文的原代码，有部分小的改动\n#| eval: false\n\ngenerate_outcomes &lt;- function(assignment.path_) {\n  Y.vec &lt;- c()\n  Y.vec.1 &lt;- mu.fixed.effect + alpha.fixed.effects[1] + delta.coef.1 * assignment.path_[1] + epsilon.noises[1]\n  Y.vec.2 &lt;- mu.fixed.effect + alpha.fixed.effects[2] + delta.coef.1 * assignment.path_[2] + delta.coef.2 * assignment.path_[1] + epsilon.noises[2]\n  Y.vec &lt;- c(Y.vec.1, Y.vec.2)\n  for (t.temp in 1:time.horizon)\n  {\n    Y.temp &lt;- mu.fixed.effect + alpha.fixed.effects[t.temp] + delta.coef.1 * assignment.path_[t.temp] + delta.coef.2 * assignment.path_[t.temp - 1] + delta.coef.3 * assignment.path_[t.temp - 2] + epsilon.noises[t.temp]\n    Y.vec &lt;- c(Y.vec, Y.temp)\n  }\n  return(Y.vec)\n}\n\ngenerate_assignments &lt;- function(time.horizon = time.horizon, randomization.points = randomization.points, re.randomization = FALSE) {\n  K &lt;- length(randomization.points)\n  # W.at.K.vec = sample(c(0,1), replace=TRUE, size=K)\n  W.at.K.vec &lt;- as.numeric(runif(K) &lt; 0.5)\n  W.vec &lt;- c()\n  if (K &gt; 1) {\n    # ===When k &lt;= K-1, append the proper assignments during each epoch\n    for (k.temp in 1:(K - 1))\n    {\n      W.vec &lt;- c(W.vec, rep(W.at.K.vec[k.temp], randomization.points[k.temp + 1] - randomization.points[k.temp]))\n    }\n    # ===When k == K, append the last epoch after the last randomization\n    W.vec &lt;- c(W.vec, rep(W.at.K.vec[K], time.horizon + 1 - randomization.points[K]))\n  }\n  if (K == 1) {\n    W.vec &lt;- rep(W.at.K.vec[K], time.horizon + 1 - randomization.points[K])\n  }\n\n  if (re.randomization == TRUE) {\n    treatment.balance &lt;- abs(sum(W.at.K.vec) - K / 2)\n    while (treatment.balance &gt; 2) {\n      W.at.K.vec &lt;- sample(c(0, 1), replace = TRUE, size = K)\n      W.vec &lt;- c()\n      if (K &gt; 1) {\n        # ===When k &lt;= K-1, append the proper assignments during each epoch\n        for (k.temp in 1:(K - 1))\n        {\n          W.vec &lt;- c(W.vec, rep(W.at.K.vec[k.temp], randomization.points[k.temp + 1] - randomization.points[k.temp]))\n        }\n        # ===When k == K, append the last epoch after the last randomization\n        W.vec &lt;- c(W.vec, rep(W.at.K.vec[K], time.horizon + 1 - randomization.points[K]))\n      }\n      if (K == 1) {\n        W.vec &lt;- rep(W.at.K.vec[K], time.horizon + 1 - randomization.points[K])\n      }\n      treatment.balance &lt;- abs(sum(W.at.K.vec) - K / 2)\n    }\n  }\n\n  return(W.vec)\n}\n\ngenerate_estimator &lt;- function(randomization.points_ = randomization.points,\n                               assignment.path_ = assignment.path,\n                               potential.outcome.path_ = potential.outcome.path,\n                               p.lag.length_ = p.lag.length) {\n  inversed.propensity.score &lt;- rep(0, p.lag.length_)\n  for (t in (p.lag.length_ + 1):time.horizon)\n  {\n    i.p.s.temp &lt;- 2^(sum(randomization.points_ %in% ((t - p.lag.length_ + 1):t)) + 1)\n    inversed.propensity.score &lt;- c(inversed.propensity.score, i.p.s.temp)\n  }\n  # inversed.propensity.score\n  positive.or.negative &lt;- rep(0, p.lag.length_)\n  for (t in (p.lag.length_ + 1):time.horizon)\n  {\n    if (sum(assignment.path_[(t - p.lag.length_):t] == 1) == (p.lag.length_ + 1)) {\n      p.or.n.temp &lt;- 1\n    } else if (sum(assignment.path_[(t - p.lag.length_):t] == 1) == 0) {\n      p.or.n.temp &lt;- -1\n    } else {\n      p.or.n.temp &lt;- 0\n    }\n    positive.or.negative &lt;- c(positive.or.negative, p.or.n.temp)\n  }\n  # positive.or.negative\n  estimator.return &lt;- sum(potential.outcome.path_ * inversed.propensity.score * positive.or.negative) / (time.horizon - p.lag.length_)\n  return(estimator.return)\n}\n\ngenerate_variance_UB_OPTDesign &lt;- function(randomization.points_ = randomization.points,\n                                           assignment.path_,\n                                           potential.outcome.path_,\n                                           p.lag.length_ = p.lag.length,\n                                           which.upper.bound = 2) {\n  K.many.randomizations &lt;- length(randomization.points_)\n  # ==Note: 1. n in the paper corresponds to (K.many.randomizations+2)\n  #        2. the paper starts with k=0; the codes starts with K.many.randomizations=1\n  observed.chunks &lt;- c()\n  for (this.randomization in 1:(K.many.randomizations + 1))\n  {\n    temp.sum &lt;- sum(potential.outcome.path_[(this.randomization * p.lag.length_ + 1):((this.randomization + 1) * p.lag.length_)])\n    observed.chunks &lt;- c(observed.chunks, temp.sum)\n  }\n  if (which.upper.bound == 1) {\n    variance.estimator &lt;- 6 * (observed.chunks[1])^2 + 6 * (observed.chunks[K.many.randomizations + 1])^2\n    for (this.randomization in 2:(K.many.randomizations))\n    {\n      if (assignment.path_[(this.randomization - 1) * p.lag.length_ + 1] == assignment.path_[this.randomization * p.lag.length_ + 1]) {\n        variance.estimator &lt;- variance.estimator + 24 * (observed.chunks[this.randomization])^2\n      }\n    }\n    for (this.randomization in 1:(K.many.randomizations))\n    {\n      if (assignment.path_[(this.randomization - 1) * p.lag.length_ + 1] == assignment.path_[this.randomization * p.lag.length_ + 1] &&\n        assignment.path_[(this.randomization + 1) * p.lag.length_ + 1] == assignment.path_[this.randomization * p.lag.length_ + 1]) {\n        variance.estimator &lt;- variance.estimator + 16 * observed.chunks[this.randomization] * observed.chunks[this.randomization + 1]\n      }\n    }\n  }\n  if (which.upper.bound == 2) {\n    variance.estimator &lt;- 8 * (observed.chunks[1])^2 + 8 * (observed.chunks[K.many.randomizations + 1])^2\n    for (this.randomization in 2:(K.many.randomizations))\n    {\n      if (assignment.path_[(this.randomization - 1) * p.lag.length_ + 1] == assignment.path_[this.randomization * p.lag.length_ + 1]) {\n        variance.estimator &lt;- variance.estimator + 32 * (observed.chunks[this.randomization])^2\n      }\n    }\n  }\n  estimator.return &lt;- variance.estimator / ((time.horizon - p.lag.length_)^2)\n  return(estimator.return)\n}\n\nmy_fisher_test &lt;- function(randomization.points_ = randomization.points,\n                           realized.outcome.path_ = realized.outcome.path,\n                           realized.HT.estimator_ = realized.HT.estimator,\n                           SAMPLE.TIMES.fisher.test_ = SAMPLE.TIMES.fisher.test) {\n  indicator.more.extreme &lt;- c()\n  for (TIMES.temp in 1:SAMPLE.TIMES.fisher.test_)\n  {\n    simulated.assignment.path &lt;- generate_assignments(time.horizon, randomization.points_)\n    simulated.HT.estimator &lt;- generate_estimator(randomization.points_, simulated.assignment.path, realized.outcome.path_)\n\n    if (abs(simulated.HT.estimator) &gt; abs(realized.HT.estimator_)) {\n      indicator.more.extreme &lt;- c(indicator.more.extreme, 1)\n    } else {\n      indicator.more.extreme &lt;- c(indicator.more.extreme, 0)\n    }\n  }\n  return(sum(indicator.more.extreme) / SAMPLE.TIMES.fisher.test_)\n}\n\n\nset.seed(111111)\ntime.horizon &lt;- 120\nno.m.carryover &lt;- 2\np.lag.length &lt;- 2\n\nSPEC.temp &lt;- 1\n\nmu.fixed.effect &lt;- 0\nalpha.fixed.effects &lt;- log(1:time.horizon) # runif(time.horizon, min = 0, max = 1) #rnorm(time.horizon, mean = 0, sd = 1)\nepsilon.noises &lt;- rnorm(time.horizon, mean = 0, sd = 1)\n\nSPEC.temp &lt;- SPEC.temp - 1\ndelta.coef.1 &lt;- (SPEC.temp &gt;= 4) * 2 + (SPEC.temp &lt; 4) * 1\ndelta.coef.2 &lt;- ((SPEC.temp %% 4) &gt;= 2) * 2 + ((SPEC.temp %% 4) &lt; 2) * 1\ndelta.coef.3 &lt;- ((SPEC.temp %% 2) &gt;= 1) * 2 + ((SPEC.temp %% 2) &lt; 1) * 1\n\n# ===Optimal design===#\nrandomization.points &lt;- c(1, seq(2 * p.lag.length + 1, time.horizon - 2 * p.lag.length + 1, by = p.lag.length))\nrandomization.points_ &lt;- randomization.points\n\nset.seed(111111)\nassignment.path &lt;- generate_assignments(time.horizon, randomization.points_)\nassignment.path_ &lt;- assignment.path\nset.seed(111111)\npotential.outcome.path &lt;- generate_outcomes(assignment.path_ = assignment.path)\npotential.outcome.path_ &lt;- potential.outcome.path\np.lag.length_ &lt;- p.lag.length\nHT.estimator &lt;- generate_estimator(randomization.points_, assignment.path, potential.outcome.path, p.lag.length)\n\npaste0(randomization.points - 1, collapse = \", \")\npaste0(assignment.path, collapse = \", \")\npaste0(potential.outcome.path, collapse = \", \")\n\ngenerate_variance_UB_OPTDesign(randomization.points, assignment.path, potential.outcome.path, p.lag.length, 1)\ngenerate_variance_UB_OPTDesign(randomization.points, assignment.path, potential.outcome.path, p.lag.length, 2)\n\nset.seed(111111)\nrealized.outcome.path &lt;- potential.outcome.path\nrealized.HT.estimator &lt;- HT.estimator\np_value &lt;- my_fisher_test(randomization.points, realized.outcome.path, realized.HT.estimator, 10000)"
  },
  {
    "objectID": "posts/switchback-experiment/index.html#测试",
    "href": "posts/switchback-experiment/index.html#测试",
    "title": "时间片轮转实验的设计和分析",
    "section": "测试",
    "text": "测试\n\nimport rpy2\nfrom rpy2 import robjects\n\ndef uniform(low=0.0, high=1.0):\n    return rpy2.robjects.r(f'runif(1, {low}, {high})')[0]\nrandom.uniform = uniform\n\ndef gauss(mu=0.0, sigma=1.0):\n    return rpy2.robjects.r(f'rnorm(1, {mu}, {sigma})')[0]\nrandom.gauss = gauss\n\ntime_horizon = 120\nm = 2\np = 0.5\nrandomization_points = generate_opt_design(time_horizon, m)\nrandomization_points == [0, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116]\n\nrpy2.robjects.r('set.seed(111111)')\nassignment_path = generate_assignment_path(randomization_points, time_horizon, m)\nassignment_path == [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n\nrpy2.robjects.r('set.seed(111111)')\ntau = [1.0, 1.0, 1.0]\nnew = generate_outcome(assignment_path, tau)\n\nold = [0.434833396762166, 4.16060424082402, 2.91790649574515, 1.39691331193944, 1.89948806343165, 1.03413730352493, 2.15911140118438, 0.726763259743356, 2.92463672663345, 4.58618087124496, 4.18384842961183, 3.1231545672721, 3.21073297148204, 3.47490426238806, 3.56474787955819, 2.54866565554569, 4.91954787364933, 4.88928321075681, 4.9743504050517, 2.70367936403911, 2.95728735703276, 2.98231567022872, 3.15047612777084, 4.75759922483731, 4.21003885524797, 4.18157462124439, 3.98018891962639, 1.16235745274864, 3.50045561681502, 4.71330628773, 7.41427588498954, 7.58288977468063, 3.66055128679812, 6.66440465565843, 7.76594302836741, 6.77364773746862, 6.81486072393467, 7.17627340036311, 6.94461498042583, 4.95567982004171, 4.134366340668, 4.77460987865831, 3.66956005339461, 4.66945960334466, 6.96777432423419, 8.82477105962375, 7.93560696678956, 7.09219170394525, 7.41808254583203, 6.72985155755407, 6.17527976040663, 5.50046812367082, 5.36712241699065, 6.55157620396049, 6.6043594442257, 4.3693445377948, 4.19116981629592, 7.11932150789102, 7.20109572388284, 4.52958187431917, 5.65838275247701, 5.59302894651198, 7.64497906403363, 7.05994580978854, 6.42953658082273, 4.93605896614588, 4.57012924056999, 5.13460375030383, 3.98956716798835, 4.5981473775092, 5.24111915320301, 6.76715647313569, 6.48136089503326, 3.3390609421107, 4.38550261887666, 4.00281801241419, 5.63593883392727, 4.51114882325828, 6.69375332242523, 6.67908667999775, 7.76266602632671, 7.45073549720124, 8.07512244608518, 8.18160320073618, 7.31424756022565, 6.33777035346318, 8.29799454274423, 7.86514809758498, 6.40034682322799, 5.61428130970244, 6.26500016482481, 5.98545855080484, 9.56853136084471, 8.27671475492981, 8.12184976500245, 7.68740785084192, 5.47432743435136, 6.66885979993122, 5.27395984151641, 5.97031188315572, 5.27049863949522, 5.25857394090524, 5.51198076300874, 7.90167715761987, 6.1132133505048, 4.17501094237396, 4.36195238641099, 5.938491498465, 3.64356687366051, 3.31405838215769, 6.66062556034787, 4.40917856947105, 7.63688284192356, 9.25694462626588, 8.20932808339688, 5.74591373194745, 6.52468337004635, 7.62154595985734, 2.27443244264151, 4.38031586688583]\nsum(1 if abs(i - j) &lt; 1.0e-10 else 0 for (i, j) in zip(new, old)) == time_horizon\n\npotential_outcome_path = new\nnew_est = estimate_tau(\n  randomization_points, \n  assignment_path, \n  potential_outcome_path, \n  m\n)\nabs(new_est[0] - 6.295755) &lt; 1.0e-6\n\nvar1 = generate_ub_variance_opt_design(randomization_points, assignment_path, potential_outcome_path, m, 1)\nvar2 = generate_ub_variance_opt_design(randomization_points, assignment_path, potential_outcome_path, m, 2)\nabs(var1 - 10.798) &lt; 1.0e-3\nabs(var2 - 10.307) &lt; 1.0e-3\n\n\nrpy2.robjects.r('set.seed(111111)')\np_value = fisher_test_switchback(\n  randomization_points,\n  assignment_path, \n  potential_outcome_path, \n  m, 10000\n)\nabs(p_value - 0.032) &lt; 1.0e-3"
  },
  {
    "objectID": "posts/minimum-sample-size/index.html",
    "href": "posts/minimum-sample-size/index.html",
    "title": "最小样本量",
    "section": "",
    "text": "在A/B测试领域，最小样本量(minimum sample size)一般用来决定实验的流量大小和持续时间。本文首先简单介绍下最小样本量的原理和推导，然后讨论一些实际应用中的问题和解决方案，最后给出代码实现和在线的demo."
  },
  {
    "objectID": "posts/minimum-sample-size/index.html#最小样本量的计算",
    "href": "posts/minimum-sample-size/index.html#最小样本量的计算",
    "title": "最小样本量",
    "section": "最小样本量的计算",
    "text": "最小样本量的计算\n最小样本量的计算公式基于以下参数：\n\n总体的方差：\\(\\sigma^2\\)。如果在实验前已经有了目标群体的数据，指标的方差就是很好的估计。特别的，如果是二分指标，比如是否付费，是否点击，总体的方差就是p * (1 - p), 这里的p是二分指标的转化率\n预期的提升MDE：\\(\\delta = \\mu_1 - \\mu_0\\), 预期实验组组的绝对提升量. 不失一般性，我们可以假设MDE &gt; 0\n设定的一类错误了\\(\\alpha\\)和二类错误率\\(\\beta\\)\n\n基于渐进正态分布假设, \\(\\delta \\sim N(\\delta, (\\frac{1}{n0} + \\frac{1}{n1}) * \\delta^2)\\)，其中\\(n_1\\), \\(n_0\\)分别是实验组和对照组的样本数。不妨定义 \\(\\frac{2}{\\widetilde{n}} = \\frac{1}{n0} + \\frac{1}{n1}\\), \\(\\widetilde{n}\\)就是\\(n_1\\), \\(n_0\\)的几何评价数，也是我们后面的最小样本数。简化后，\\(\\delta \\sim N(\\delta, \\frac{2\\delta^2}{\\widetilde{n}})\\)\n\n在H0下，实际的\\(\\delta = 0\\), 为了不犯一类错误，实际观察到的\\(\\bar{x}\\) 满足\n\n\\[ \\frac{\\bar{x} - 0}{(\\frac{2\\sigma^2}{\\widetilde{n}})}&lt;= \\Phi(1 - \\frac{\\alpha}{2}) \\]\n\n在H1下，实际的\\(\\delta = MDE\\), 为了不犯二类错误，实际观察到的\\(\\bar{x}\\) 满足\n\n\\[ \\frac{\\bar{x} - \\delta}{\\sqrt{\\frac{2\\sigma^2}{\\widetilde{n}}}} &gt;= \\Phi(\\beta) \\]\n两边乘以-1，有\n\\[ \\Rightarrow \\frac{\\delta - \\bar{x}}{\\sqrt{\\frac{2\\sigma^2}{\\widetilde{n}}}} &lt;=  \\Phi(1 - \\beta) \\]\n和H0的表达式相加，消去了\\(\\bar{x}\\)，有\n\\[ \\frac{\\delta}{\\sqrt{\\frac{2\\sigma^2}{\\widetilde{n}}}} &lt;= \\Phi(1 - \\beta) + \\Phi(1 - \\frac{\\alpha}{2}) \\]\n\\[ \\Rightarrow \\widetilde{n} &gt;= 2(\\Phi(\\beta) + \\Phi(1 - \\frac{\\alpha}{2}))^2 * \\frac{\\sigma^2}{\\delta^2} \\]\n记\\(K(\\alpha, \\beta) = 2(\\Phi(\\beta) + \\Phi(1 - \\frac{\\alpha}{2}))^2\\), 有\n\\[ \\widetilde{n} &gt;= K(\\alpha, \\beta) * \\frac{\\sigma^2}{\\delta^2} \\]\n这个公式非常符合直觉：\n\n最小样本量\\(\\widetilde{n}\\)和\\(K(\\alpha, \\beta)\\)成正比，\\(\\alpha\\)和\\(\\beta\\)越小，需要的样本量越大\n最小样本量\\(\\widetilde{n}\\)和\\(\\delta^2\\)成反比，实验的效应如果很明显，那么需要的样本量会比较少\n最小样本量\\(\\widetilde{n}\\)和\\(\\sigma^2\\)成正比，指标的波动性越大，需要的样本量越大\n\n最小样本量(minimum sample size)是A/B测试领域经常涉及到的话题,它有个很出名的简化了的经验公式：\n一般默认\\(\\alpha = 0.05\\)和\\(\\beta = 0.20\\)\n\\[ K(\\alpha, \\beta) = 2 * (\\Phi(1 - 0.2) + \\Phi(1 - 0.05 / 2)) \\] \\[ = 2 * (qnorm(1 - 0.2) + qnorm(1 - 0.05 / 2)) ** 2 \\approx 16 \\]\n这就是我们经常见到的这个经验公式 \\[ n = 16 * \\sigma^2 / \\delta^2 \\]\n关于这个公式，有几个值得注意的点：\n\nn是实验组对照组单独的，即实验组和对照组分别至少需要n, 如果实验组和对照组不是等分的，这里的n最接近实验组和对照组人数的几何平均\nn是相对实验的分析粒度的，除了部分时间片轮转实验以外，大部分实验的粒度都不是按天的\n\n对于0/1指标，\\(\\sigma^2 = p * (1 - p)\\), \\(\\delta\\)就是预期的转化率的绝对提升。举个例子，假设大盘的点击率是0.05，现在算法有了迭代，预期点击率的相对提升1%，绝对提升0.05 * 0.01 = 0.0005. 根据上面的公式，最小样本量 n = 16 * 0.05 * (1 - 0.05) / (0.0005 ** 2) = 3040000. 总样本接近6百万（3040000 * 2)。这个简单的例子可以帮我们一个迷思：相对于经典实验，比如心理学实验、农业实验、药物实验。互联网实验的样本大了很多数量级，那经典的假设检验和p value还有效吗？几百万几千万的样本是不是足够检验出任意小的差异？从上面的具体例子看，这个说法大部分是不成立的，在我们的设定中，新算法能相对提升1%，这不是一个很容易做的事情，但这里的样本量已经达成了600万。"
  },
  {
    "objectID": "posts/规划的系列文章.html",
    "href": "posts/规划的系列文章.html",
    "title": "规划的系列文章",
    "section": "",
    "text": "A/B test:\n\n实验前\n\nhash是合适的随机数生成器吗？\n实验设计的问题：完全随机、分层/区组随机\n再随机化(Rerandomization)实验的设计和数据处理\n平不平的问题：除非你的分流足够随机？\n最小样本量的估计和一些实际的问题\n\n实验数据处理\n\n0/1、计数和连续变量的分析\nClustered standard errors的估计\n分位数指标的处理\n时间片轮转实验的设计和处理\nbootstrap方法\n\n基于spark的AB实验数据处理引擎\n基于多臂老虎机的实验引擎\n\n因果推断评估：观察性数据的估计\n\n回归、匹配和psm\nSCM、DID和SDID\n\n因果推断估计：个体处理效应估计\n\nuplift/causal forest的评估问题\nuplift/causal forest的spark实现\nDRL、DML及其他\n\n强化学习\n\nQ学习\n策略学习\n\nppo\n\n\nspark-causalml系列"
  },
  {
    "objectID": "posts/规划的系列文章.html#规划题目",
    "href": "posts/规划的系列文章.html#规划题目",
    "title": "规划的系列文章",
    "section": "",
    "text": "A/B test:\n\n实验前\n\nhash是合适的随机数生成器吗？\n实验设计的问题：完全随机、分层/区组随机\n再随机化(Rerandomization)实验的设计和数据处理\n平不平的问题：除非你的分流足够随机？\n最小样本量的估计和一些实际的问题\n\n实验数据处理\n\n0/1、计数和连续变量的分析\nClustered standard errors的估计\n分位数指标的处理\n时间片轮转实验的设计和处理\nbootstrap方法\n\n基于spark的AB实验数据处理引擎\n基于多臂老虎机的实验引擎\n\n因果推断评估：观察性数据的估计\n\n回归、匹配和psm\nSCM、DID和SDID\n\n因果推断估计：个体处理效应估计\n\nuplift/causal forest的评估问题\nuplift/causal forest的spark实现\nDRL、DML及其他\n\n强化学习\n\nQ学习\n策略学习\n\nppo\n\n\nspark-causalml系列"
  },
  {
    "objectID": "posts/规划的系列文章.html#核心原则",
    "href": "posts/规划的系列文章.html#核心原则",
    "title": "规划的系列文章",
    "section": "核心原则",
    "text": "核心原则\n\n重点关心方法/算法的核心逻辑、背后的直觉和实际的影响，而不是推导/理论的的严格性\n突出算法及其相应的代码实现问题，强调代码工程问题\n在线的交互式的演示"
  },
  {
    "objectID": "posts/post-with-online-code/index.html",
    "href": "posts/post-with-online-code/index.html",
    "title": "Post With Live Code",
    "section": "",
    "text": "python -m pip install shinylive\nquarto add quarto-ext/shinylive\nquarto install tinytex # pdf"
  },
  {
    "objectID": "posts/post-with-online-code/index.html#参考文献引用",
    "href": "posts/post-with-online-code/index.html#参考文献引用",
    "title": "Post With Live Code",
    "section": "参考文献引用",
    "text": "参考文献引用\n引用 Liu et al. (2019) 的文章\n\nReferences\n\n\nLiu, Min, Xiaohui Sun, Maneesh Varshney, and Ya Xu. 2019. “Large-Scale Online Experimentation with Quantile Metrics.” arXiv Preprint arXiv:1903.08762."
  },
  {
    "objectID": "posts/quantile-ab-test/index.html",
    "href": "posts/quantile-ab-test/index.html",
    "title": "A/B test中分位数指标的处理",
    "section": "",
    "text": "在A/B test，分位数指标主要见于两个场景：\n本文主要基于linkedin Liu et al. (2019) 的Large-Scale Online Experimentation with Quantile Metrics和Spotify Schultzberg and Ankargren (2022) 的Resampling-free bootstrap inference for quantiles，同时也会提到和计量经济学的分位数回归的对比，来讨论A/B中分位数指标的处理。"
  },
  {
    "objectID": "posts/quantile-ab-test/index.html#定义和符号",
    "href": "posts/quantile-ab-test/index.html#定义和符号",
    "title": "A/B test中分位数指标的处理",
    "section": "定义和符号",
    "text": "定义和符号\n假设\\(X\\)是连续的随机变量，\\(x_0, x_1, x_2, ..., x_{n - 1}\\)是\\(X\\)的容量为N的随机独立样本(i.i.d)，\\(f(x)\\)是概率密度函数，\\(F(x)\\)累计概率分布。样本的第tau(0 &lt;= tau &lt;= 1)分位数记为\\(\\hat{Q}(\\tau)\\), 相应的总体分位数记为 \\(Q(\\tau)\\)。\n\\[ \\hat{Q}(\\tau) \\sim \\mathcal{N}(Q(\\tau), \\frac{\\tau * (1 - \\tau)}{N * f(Q(\\tau) ^ 2)}) \\]\n这里有两个需要从样本中估计的量\\(Q(\\tau)\\)和相应的\\(f(Q(\\tau)\\)。\\(Q(\\tau)\\)比较容易（尽管有9种估计方式，见wiki，或者R的quantile的文档），\\(f(Q(\\tau)\\)涉及到核密度估计, 下面稍微展开一下。\n\\[ {\\widehat {f}}_{h}(x)={\\frac {1}{n}}\\sum _{i=1}^{n}K_{h}(x-x_{i})={\\frac {1}{nh}}\\sum _{i=1}^{n}K{\\Big (}{\\frac {x-x_{i}}{h}}{\\Big )} \\]\n这里有两个参数: 核\\(K\\)和带宽h。quora、wish采用了以下方式\n\\[f(Q(\\tau) = \\frac{h}{\\hat{Q}(\\tau + h / 2) - \\hat{Q}(\\tau - h / 2)}\\]\n其中带宽h根据具体的业务情况选择，大致的原则是”覆盖到整体的0.1%到1%的数据”。python的stat.model、R的quantreg、stata采用了更成熟的核密度估计方法，下面的代码实现了stat.model的默认估计方法，它是基于Epanechnikov核和hall_sheather带宽选择法。\n\nfrom typing import List, Callable\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import norm\n\ndef hall_sheather(n, tau, alpha=.05):\n    z = norm.ppf(tau)\n    num = 1.5 * norm.pdf(z)**2.\n    den = 2. * z**2. + 1.\n    h = n**(-1. / 3) * norm.ppf(1. - alpha / 2.)**(2./3) * (num / den)**(1./3)\n    return h\n\n# stat.model中quantreg的默认估计方法\ndef desity_at_quantile(\n    x: List[float], \n    tau: float, \n    kernel: Callable[float, float] = lambda u: 3. / 4 * (1-u**2) * np.where(np.abs(u) &lt;= 1, 1, 0), \n    bandwidth: Callable[[float, float], float] = hall_sheather\n):\n    assert len(x) &gt; 0\n    assert tau &gt;= 0.0 and tau &lt;= 1.0\n    \n    x = np.array(x)\n    nobs = len(x)\n    e = x - np.quantile(x, tau)\n\n    iqre = stats.scoreatpercentile(e, 75) - stats.scoreatpercentile(e, 25)\n    h = bandwidth(nobs, tau)\n    # Greene (2008, p.407) writes that Stata 6 uses this bandwidth:\n    # h = 0.9 * np.std(e) / (nobs**0.2)\n    # Instead, we calculate bandwidth as in Stata 12\n    h = min(np.std(x), iqre / 1.34) * (norm.ppf(tau + h) - norm.ppf(tau - h))\n    return 1. / (nobs * h) * np.sum(kernel(e / h))\n\n# quora的估计\ndef desity_at_quantile_naive(x: List[float], tau: float, bandwidth: float = 0.005): \n    return bandwidth / (np.quantile(x, tau + bandwidth / 2) - np.quantile(x, tau - bandwidth / 2))\n\nnp.random.seed(0)\nx = np.random.normal(size = 100000)\ndq_sum, ndq_sum = 0.0, 0.0\n\nn = 0.0\nfor i in range(1, 100):\n    tau = i * 0.01\n    tq = norm.pdf(norm.ppf(tau)) \n    sq = desity_at_quantile(x, tau)\n    dq = sq - tq\n    dq_sum += abs(dq)\n    nsq = desity_at_quantile_naive(x, tau)\n    ndq = nsq - tq\n    ndq_sum += abs(ndq)\n    n += 1.0\n    # print(f\"q: {q:6.4f}, true density: {tq:6.4f}, epa: {sq:6.4f}, naive: {nsq:6.4f}, epa error: {dq:8.4f}, naive error: {ndq:8.4f}\")\n\nprint(f\"mean absolute error, epa: { (dq_sum / n):6.4f}, naive: { (ndq_sum / n):6.4f}\")\n\nmean absolute error, epa: 0.0029, naive: 0.0111\n\n\n可以看到，相比于quora的方案，stat.model的核密度估计效果更好一些。\n现在考虑实验的分位数处理效应QTE，或者\\(\\Delta\\), 显然有\n\\[ QTE \\sim \\mathcal{N}(\\hat{Q}_t(\\tau) - \\hat{Q}_c(\\tau), \\sqrt{Var(\\hat{Q_t(\\tau)}) + Var(\\hat{Q_c(\\tau)}}) \\]\n这个结果对实验分组变量D（表示用户是实验组还是对照组）做分位数回归的结果是一致的。"
  },
  {
    "objectID": "posts/quantile-ab-test/index.html#非聚类情况的分位数检验",
    "href": "posts/quantile-ab-test/index.html#非聚类情况的分位数检验",
    "title": "A/B test中分位数指标的处理",
    "section": "非聚类情况的分位数检验",
    "text": "非聚类情况的分位数检验\n我们先考虑非聚类样本的情况，或者说随机单元等于分析单元的情况。假设有n个独立样本\\(X = {x_0, x_1, x_2, ..., x_{n - 1}}\\), 记为\\(q_X(\\tau)\\)为X的第\\(\\tau\\)个分位数，相应的\\(\\hat{q}_X(\\tau)\\)为样本tau分位数\n\\[ q(\\tau) \\sim \\mathcal{N}(\\hat{q}(\\tau), \\frac{\\tau * (1 - \\tau)}{n * f(\\hat{q}(\\tau))^2})) \\]\n\nfrom typing import List, Callable\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\ndef quant_test(x: List[float], y: List[float], tau: float, conf_level: float = 0.95, alternative: str = 'two-sided'):\n    def estimate_var(x, tau):\n        return tau * (1 - tau) / (desity_at_quantile(x, tau) ** 2) / len(x)\n\n    assert tau &gt;= 0.0 and tau &lt;= 1.0\n\n    qx = np.quantile(x, tau)\n    qy = np.quantile(y, tau)\n    qte = qy - qx\n    stderr = np.sqrt(estimate_var(x, tau) + estimate_var(y, tau))\n\n    return (qte, stderr)\n    \n\n(n0, mu0, sd0) = (30000, 0.0, 1.0)\n(n1, mu1, sd1) = (31000, 0.2, 0.8)\ntau = 0.7\ntq = norm.pdf(tau, mu1, sd1) - norm.pdf(tau, mu0, sd0)  # 真实的qte\nx = np.random.normal(mu0, sd0, n0)\ny = np.random.normal(mu1, sd1, n1) \n# (0.09097998111909678, 2.830236593580436)\n(qte, stderr) = quant_test(x, y, tau)\nprint(f\"quant_test, true qte: {tq:5.3f}, qte: {qte:5.3f}, std.err:{stderr:10.6f}\")\n\nquant_test, true qte: 0.098, qte: 0.095, std.err:  0.009479\n\ndata = pd.DataFrame({'y': np.concatenate([x, y]), 't': np.repeat([0, 1], [n0, n1])})\nmod = smf.quantreg(\"y ~ t\", data)\nres = mod.fit(q=tau, vcov='iid')\n# print(res.summary())\nprint(f\"stat model quant regression, true qte: {tq:5.3f}, qte: {res.params['t']:5.3f}, std.err:{res.bse['t']:10.6f}\")\n\nstat model quant regression, true qte: 0.098, qte: 0.095, std.err:  0.009233\n\nimport numpy as np\nimport warnings\nimport scipy.stats as stats\nfrom numpy.linalg import pinv\nfrom scipy.stats import norm\n\nq = tau\nb0 = np.quantile(x, q)\nbeta = np.array([b0, qte])\n\nbeta = res.params.to_numpy()\nexog = np.stack([np.repeat([1.0], [n0 + n1]), np.repeat([0.0, 1.0], [n0, n1])], axis=1)\nendog = data['y'].to_numpy()\n\nkernel = lambda u: 3. / 4 * (1-u**2) * np.where(np.abs(u) &lt;= 1, 1, 0)\nbandwidth = hall_sheather\n\nnobs = endog.shape[0]\ne = endog - np.dot(exog, beta)\n# Greene (2008, p.407) writes that Stata 6 uses this bandwidth:\n# h = 0.9 * np.std(e) / (nobs**0.2)\n# Instead, we calculate bandwidth as in Stata 12\niqre = stats.scoreatpercentile(e, 75) - stats.scoreatpercentile(e, 25)\nh = bandwidth(nobs, q)\nh = min(np.std(endog), iqre / 1.34) * (norm.ppf(q + h) - norm.ppf(q - h))\nfhat0 = 1. / (nobs * h) * np.sum(kernel(e / h))\n\nd = np.where(e &gt; 0, (q/fhat0)**2, ((1-q)/fhat0)**2)\nxtxi = pinv(np.dot(exog.T, exog))\nxtdx = np.dot(exog.T * d[np.newaxis, :], exog)\nvcov = xtxi @ xtdx @ xtxi\n\nvcov = (1. / fhat0)**2 * q * (1 - q) * pinv(np.dot(exog.T, exog))\nnp.sqrt(vcov[1, 1])\n\n    \n# pinv(np.dot(exog.T, exog)) * (n0 + n1)\n# exog = \n# fhat0 = desity_at_quantile()\n\n0.009232502428670537\n\n\n这两者的结果比较一致的，但有一点小的差别。\n\\[\n    \\sum_{g=1}^{G} X_g^T e_g e_g^T X_g =\n    \\left[\n      \\begin{array}{cccc}\n          e_1^T X_1\\\\\n          e_2^T X_2\\\\\n          \\vdots\\\\\n          e_G^T X_G\\\\\n      \\end{array}\n    \\right]^T\n    \\left[\n      \\begin{array}{cccc}\n          e_1^T X_1\\\\\n          e_2^T X_2\\\\\n          \\vdots\\\\\n          e_G^T X_G\\\\\n      \\end{array}\n    \\right]\n    \\]\n\n# n0 &lt;- 1000\n# mu0 &lt;- 0.0\n# sd0 &lt;- 1.0\n# n1 &lt;- 1100\n# mu1 &lt;- 0.2\n# sd1 &lt;- 0.8\n\n# y0 &lt;- rnorm(n0, mu0, sd0)\n# y1 &lt;- rnorm(n1, mu1, sd1)\n# # x &lt;- rep(0:1, c(n0, n1))\n# x &lt;- matrix(rep(c(1, 0, 1), c(n0 + n1, n0, n1)), ncol = 2)\n# # x &lt;- matrix(rep(c(0, 1), c(n0, n1)), ncol = 1)\n# y &lt;- matrix(c(y0, y1), ncol = 1)\n\n\n# wquantile &lt;- function(x, y, tau = 0.5) {\n#   o &lt;- order(y / x)\n#   b &lt;- (y / x)[o]\n#   w &lt;- abs(x[o])\n#   k &lt;- sum(cumsum(w) &lt; ((tau - 0.5) * sum(x) + 0.5 * sum(w)))\n#   #   k &lt;- sum(cumsum(w) &lt; ((tau - 1.) * sum(x) + tau * sum(w)))\n#   list(coef = b[k + 1], k = o[k + 1])\n# }\n\n# rqx &lt;- function(x, y, tau = 0.5, max.it = 50) { # Barrodale and Roberts -- lite\n#   p &lt;- ncol(x)\n#   n &lt;- nrow(x)\n#   h &lt;- sample(1:n, size = p) # Phase I -- find a random (!) initial basis\n#   it &lt;- 0\n#   repeat {\n#     it &lt;- it + 1\n#     Xhinv &lt;- solve(x[h, ])\n#     bh &lt;- Xhinv %*% y[h]\n#     rh &lt;- y - x %*% bh\n#     # find direction of steepest descent along one of the edges\n#     g &lt;- -t(Xhinv) %*% t(x[-h, ]) %*% c(tau - (rh[-h] &lt; 0))\n#     g &lt;- c(g + (1 - tau), -g + tau)\n#     ming &lt;- min(g)\n#     if (ming &gt;= 0 || it &gt; max.it) break\n#     h.out &lt;- seq(along = g)[g == ming]\n#     sigma &lt;- ifelse(h.out &lt;= p, 1, -1)\n#     if (sigma &lt; 0) h.out &lt;- h.out - p\n#     d &lt;- sigma * Xhinv[, h.out]\n#     # find step length by one-dimensional wquantile minimization\n#     xh &lt;- x %*% d\n#     step &lt;- wquantile(xh, rh, tau)\n#     h.in &lt;- step$k\n#     h &lt;- c(h[-h.out], h.in)\n#   }\n#   if (it &gt; max.it) warning(\"non-optimal solution: max.it exceeded\")\n#   return(bh)\n# }\n\n# meketon &lt;- function(x, y, eps = 1e-04, beta = 0.97) {\n#   f &lt;- lm.fit(x, y)\n#   n &lt;- length(y)\n#   w &lt;- rep(0, n)\n#   d &lt;- rep(1, n)\n#   its &lt;- 0\n#   while (sum(abs(f$resid)) - crossprod(y, w) &gt; eps) {\n#     its &lt;- its + 1\n#     s &lt;- f$resid * d\n#     alpha &lt;- max(pmax(s / (1 - w), -s / (1 + w)))\n#     w &lt;- w + (beta / alpha) * s\n#     d &lt;- pmin(1 - w, 1 + w)^2\n#     f &lt;- lm.wfit(x, y, d)\n#   }\n#   list(coef = f$coef, iterations = its)\n# }\n\n# tau &lt;- 0.3\n# # wquantile(t, xy, tau)$coef\n# # quantile(y, tau) - quantile(x, tau)\n\n# # sprintf(\"tau: %.4f, est: %.4f, act: %.4f\", tau, wquantile(t, xy, tau)$coef, quantile(y, tau) - quantile(x, tau))\n# meketon(x, y)$coef\n# c(quantile(y0, tau), quantile(y1, tau) - quantile(y0, tau))\n\n# sprintf(\"tau: %.4f, est: %.4f, act: %.4f\", tau, rqx(x, y, tau), quantile(y1, tau) - quantile(y0, tau))\n\n\nReferences\n\n\nLiu, Min, Xiaohui Sun, Maneesh Varshney, and Ya Xu. 2019. “Large-Scale Online Experimentation with Quantile Metrics.” arXiv Preprint arXiv:1903.08762.\n\n\nSchultzberg, Mårten, and Sebastian Ankargren. 2022. “Resampling-Free Bootstrap Inference for Quantiles.” In Proceedings of the Future Technologies Conference, 548–62. Springer."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A/B、因果推断和强化学习",
    "section": "",
    "text": "规划的系列文章\n\n\n\n\n\n\n\noutline\n\n\n\n\n\n\n\n\n\n\n\n2023-09-23\n\n\n张振昊\n\n\n\n\n\n\n  \n\n\n\n\n最小样本量\n\n\n\n\n\n\n\nA/B test\n\n\nminimum sample size\n\n\n\n\n\n\n\n\n\n\n\n2023-09-19\n\n\n张振昊\n\n\n\n\n\n\n  \n\n\n\n\n时间片轮转实验的设计和分析\n\n\n\n\n\n\n\nA/B test\n\n\nswitchback experiment\n\n\n\n\n\n\n\n\n\n\n\n2023-09-19\n\n\n张振昊\n\n\n\n\n\n\n  \n\n\n\n\nA/B test中分位数指标的处理\n\n\n\n\n\n\n\nA/B test\n\n\nquantile test\n\n\n\n\n\n\n\n\n\n\n\n2023-09-18\n\n\n张振昊\n\n\n\n\n\n\n  \n\n\n\n\n分位数回归的原理、代码实现和聚类标准误\n\n\n\n\n\n\n\nA/B test\n\n\nquantile regression\n\n\nclustered standard errors\n\n\n\n\n\n\n\n\n\n\n\n2023-09-18\n\n\n张振昊\n\n\n\n\n\n\n  \n\n\n\n\nPost With Live Code\n\n\n\n\n\n\n\nshinylive\n\n\nquarto\n\n\n\n\n\n\n\n\n\n\n\n2023-09-15\n\n\n张振昊\n\n\n\n\n\n\nNo matching items"
  }
]